{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNWMbYw25loist1My0ClH9O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wx3w74ROxBFS","executionInfo":{"status":"ok","timestamp":1665977908007,"user_tz":-330,"elapsed":16032,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"fc0b42a0-5865-4152-c860-db24dd14d33c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'onnx'...\n","remote: Enumerating objects: 35562, done.\u001b[K\n","remote: Counting objects: 100% (66/66), done.\u001b[K\n","remote: Compressing objects: 100% (53/53), done.\u001b[K\n","remote: Total 35562 (delta 20), reused 40 (delta 13), pack-reused 35496\u001b[K\n","Receiving objects: 100% (35562/35562), 24.57 MiB | 17.89 MiB/s, done.\n","Resolving deltas: 100% (20216/20216), done.\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnx\n","  Downloading onnx-1.12.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n","\u001b[K     |████████████████████████████████| 13.1 MB 13.5 MB/s \n","\u001b[?25hCollecting onnxruntime\n","  Downloading onnxruntime-1.12.1-cp37-cp37m-manylinux_2_27_x86_64.whl (4.9 MB)\n","\u001b[K     |████████████████████████████████| 4.9 MB 46.1 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.1.1)\n","Requirement already satisfied: protobuf<=3.20.1,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.12.2->onnx) (1.15.0)\n","Collecting coloredlogs\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[K     |████████████████████████████████| 46 kB 3.8 MB/s \n","\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.7.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (21.3)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.12)\n","Collecting humanfriendly>=9.1\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->onnxruntime) (3.0.9)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy->onnxruntime) (1.2.1)\n","Installing collected packages: humanfriendly, coloredlogs, onnxruntime, onnx\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnx-1.12.0 onnxruntime-1.12.1\n"]}],"source":["!git clone https://github.com/onnx/onnx.git\n","#!git https://github.com/microsoft/onnxruntime.git\n","!pip install onnx onnxruntime"]},{"cell_type":"code","source":["!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7tD3xm6Ly3px","executionInfo":{"status":"ok","timestamp":1665977908533,"user_tz":-330,"elapsed":542,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"9a06dede-f968-4599-d615-4a57a3a1be02"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'lightweight-human-pose-estimation.pytorch'...\n","remote: Enumerating objects: 120, done.\u001b[K\n","remote: Total 120 (delta 0), reused 0 (delta 0), pack-reused 120\u001b[K\n","Receiving objects: 100% (120/120), 222.76 KiB | 9.68 MiB/s, done.\n","Resolving deltas: 100% (53/53), done.\n"]}]},{"cell_type":"code","source":["!pip install -r /content/lightweight-human-pose-estimation.pytorch/requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLLDpUoA2D_x","executionInfo":{"status":"ok","timestamp":1665977919242,"user_tz":-330,"elapsed":10714,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"313c224f-c0f1-4638-d8a8-3e1e64dcdb97"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 1)) (1.12.1+cu113)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from -r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (0.13.1+cu113)\n","Collecting pycocotools==2.0.0\n","  Downloading pycocotools-2.0.0.tar.gz (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 13.7 MB/s \n","\u001b[?25hRequirement already satisfied: opencv-python>=3.4.0.14 in /usr/local/lib/python3.7/dist-packages (from -r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 4)) (4.6.0.66)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 5)) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 1)) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->-r /content/lightweight-human-pose-estimation.pytorch/requirements.txt (line 2)) (3.0.4)\n","Building wheels for collected packages: pycocotools\n","  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pycocotools: filename=pycocotools-2.0.0-cp37-cp37m-linux_x86_64.whl size=265116 sha256=951fc97bc3579f577b6992bcf82a7249528558918e47865aa70d4f14a534f472\n","  Stored in directory: /root/.cache/pip/wheels/4d/50/dc/e1f07e9eb5678a0ee21bc091220f1f3806ba8e48ef3f2083cb\n","Successfully built pycocotools\n","Installing collected packages: pycocotools\n","  Attempting uninstall: pycocotools\n","    Found existing installation: pycocotools 2.0.5\n","    Uninstalling pycocotools-2.0.5:\n","      Successfully uninstalled pycocotools-2.0.5\n","Successfully installed pycocotools-2.0.0\n"]}]},{"cell_type":"code","source":["%cd /content/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sT6S8sVD5tMH","executionInfo":{"status":"ok","timestamp":1665976080863,"user_tz":-330,"elapsed":821,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"fd513fbb-7ef9-48d6-90eb-ad2d7616e92a"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"av-hNZbS5qTJ","executionInfo":{"status":"ok","timestamp":1665977922959,"user_tz":-330,"elapsed":3734,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"f72cc2af-0f7c-44cb-c89f-3978a51ed8db"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-10-17 03:38:39--  https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth\n","Resolving download.01.org (download.01.org)... 23.222.54.105, 2a02:26f0:b200:39d::4b21, 2a02:26f0:b200:384::4b21\n","Connecting to download.01.org (download.01.org)|23.222.54.105|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 87959810 (84M) [application/octet-stream]\n","Saving to: ‘checkpoint_iter_370000.pth’\n","\n","checkpoint_iter_370 100%[===================>]  83.88M  40.4MB/s    in 2.1s    \n","\n","2022-10-17 03:38:42 (40.4 MB/s) - ‘checkpoint_iter_370000.pth’ saved [87959810/87959810]\n","\n"]}]},{"cell_type":"code","source":["import io\n","import numpy as np\n","\n","from torch import nn\n","import torch.utils.model_zoo as model_zoo\n","import torch.onnx"],"metadata":{"id":"P5Dy8mGy6sno","executionInfo":{"status":"ok","timestamp":1665977924485,"user_tz":-330,"elapsed":1529,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%cd /content/lightweight-human-pose-estimation.pytorch\n","!python scripts/convert_to_onnx.py --checkpoint-path /content/checkpoint_iter_370000.pth"],"metadata":{"id":"K4dh1m8ey3xW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665976977855,"user_tz":-330,"elapsed":2262,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"307c01ef-df5d-49b1-8151-6b3cce936933"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/lightweight-human-pose-estimation.pytorch\n","Traceback (most recent call last):\n","  File \"scripts/convert_to_onnx.py\", line 5, in <module>\n","    from ltwt_pose_models.with_mobilenet import PoseEstimationWithMobileNet\n","ModuleNotFoundError: No module named 'ltwt_pose_models'\n"]}]},{"cell_type":"code","source":["from torch import nn\n","\n","\n","def conv(in_channels, out_channels, kernel_size=3, padding=1, bn=True, dilation=1, stride=1, relu=True, bias=True):\n","    modules = [nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)]\n","    if bn:\n","        modules.append(nn.BatchNorm2d(out_channels))\n","    if relu:\n","        modules.append(nn.ReLU(inplace=True))\n","    return nn.Sequential(*modules)\n","\n","\n","def conv_dw(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n","        nn.BatchNorm2d(in_channels),\n","        nn.ReLU(inplace=True),\n","\n","        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True),\n","    )\n","\n","\n","def conv_dw_no_bn(in_channels, out_channels, kernel_size=3, padding=1, stride=1, dilation=1):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation=dilation, groups=in_channels, bias=False),\n","        nn.ELU(inplace=True),\n","\n","        nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n","        nn.ELU(inplace=True),\n","    )\n"],"metadata":{"id":"qP7sG_d0AHX5","executionInfo":{"status":"ok","timestamp":1665977930226,"user_tz":-330,"elapsed":10,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","\n","\n","class Cpm(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.align = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n","        self.trunk = nn.Sequential(\n","            conv_dw_no_bn(out_channels, out_channels),\n","            conv_dw_no_bn(out_channels, out_channels),\n","            conv_dw_no_bn(out_channels, out_channels)\n","        )\n","        self.conv = conv(out_channels, out_channels, bn=False)\n","\n","    def forward(self, x):\n","        x = self.align(x)\n","        x = self.conv(x + self.trunk(x))\n","        return x\n","\n","\n","class InitialStage(nn.Module):\n","    def __init__(self, num_channels, num_heatmaps, num_pafs):\n","        super().__init__()\n","        self.trunk = nn.Sequential(\n","            conv(num_channels, num_channels, bn=False),\n","            conv(num_channels, num_channels, bn=False),\n","            conv(num_channels, num_channels, bn=False)\n","        )\n","        self.heatmaps = nn.Sequential(\n","            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n","            conv(512, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n","        )\n","        self.pafs = nn.Sequential(\n","            conv(num_channels, 512, kernel_size=1, padding=0, bn=False),\n","            conv(512, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n","        )\n","\n","    def forward(self, x):\n","        trunk_features = self.trunk(x)\n","        heatmaps = self.heatmaps(trunk_features)\n","        pafs = self.pafs(trunk_features)\n","        return [heatmaps, pafs]\n","\n","\n","class RefinementStageBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.initial = conv(in_channels, out_channels, kernel_size=1, padding=0, bn=False)\n","        self.trunk = nn.Sequential(\n","            conv(out_channels, out_channels),\n","            conv(out_channels, out_channels, dilation=2, padding=2)\n","        )\n","\n","    def forward(self, x):\n","        initial_features = self.initial(x)\n","        trunk_features = self.trunk(initial_features)\n","        return initial_features + trunk_features\n","\n","\n","class RefinementStage(nn.Module):\n","    def __init__(self, in_channels, out_channels, num_heatmaps, num_pafs):\n","        super().__init__()\n","        self.trunk = nn.Sequential(\n","            RefinementStageBlock(in_channels, out_channels),\n","            RefinementStageBlock(out_channels, out_channels),\n","            RefinementStageBlock(out_channels, out_channels),\n","            RefinementStageBlock(out_channels, out_channels),\n","            RefinementStageBlock(out_channels, out_channels)\n","        )\n","        self.heatmaps = nn.Sequential(\n","            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n","            conv(out_channels, num_heatmaps, kernel_size=1, padding=0, bn=False, relu=False)\n","        )\n","        self.pafs = nn.Sequential(\n","            conv(out_channels, out_channels, kernel_size=1, padding=0, bn=False),\n","            conv(out_channels, num_pafs, kernel_size=1, padding=0, bn=False, relu=False)\n","        )\n","\n","    def forward(self, x):\n","        trunk_features = self.trunk(x)\n","        heatmaps = self.heatmaps(trunk_features)\n","        pafs = self.pafs(trunk_features)\n","        return [heatmaps, pafs]\n","\n","\n","class PoseEstimationWithMobileNet(nn.Module):\n","    def __init__(self, num_refinement_stages=1, num_channels=128, num_heatmaps=19, num_pafs=38):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            conv(     3,  32, stride=2, bias=False),\n","            conv_dw( 32,  64),\n","            conv_dw( 64, 128, stride=2),\n","            conv_dw(128, 128),\n","            conv_dw(128, 256, stride=2),\n","            conv_dw(256, 256),\n","            conv_dw(256, 512),  # conv4_2\n","            conv_dw(512, 512, dilation=2, padding=2),\n","            conv_dw(512, 512),\n","            conv_dw(512, 512),\n","            conv_dw(512, 512),\n","            conv_dw(512, 512)   # conv5_5\n","        )\n","        self.cpm = Cpm(512, num_channels)\n","\n","        self.initial_stage = InitialStage(num_channels, num_heatmaps, num_pafs)\n","        self.refinement_stages = nn.ModuleList()\n","        for idx in range(num_refinement_stages):\n","            self.refinement_stages.append(RefinementStage(num_channels + num_heatmaps + num_pafs, num_channels,\n","                                                          num_heatmaps, num_pafs))\n","\n","    def forward(self, x):\n","        backbone_features = self.model(x)\n","        backbone_features = self.cpm(backbone_features)\n","\n","        stages_output = self.initial_stage(backbone_features)\n","        for refinement_stage in self.refinement_stages:\n","            stages_output.extend(\n","                refinement_stage(torch.cat([backbone_features, stages_output[-2], stages_output[-1]], dim=1)))\n","\n","        return stages_output"],"metadata":{"id":"fDWXiFfRAI8z","executionInfo":{"status":"ok","timestamp":1665977934213,"user_tz":-330,"elapsed":592,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import collections\n","\n","\n","def load_state(net, checkpoint):\n","    source_state = checkpoint['state_dict']\n","    target_state = net.state_dict()\n","    new_target_state = collections.OrderedDict()\n","    for target_key, target_value in target_state.items():\n","        if target_key in source_state and source_state[target_key].size() == target_state[target_key].size():\n","            new_target_state[target_key] = source_state[target_key]\n","        else:\n","            new_target_state[target_key] = target_state[target_key]\n","            print('[WARNING] Not found pre-trained parameters for {}'.format(target_key))\n","\n","    net.load_state_dict(new_target_state)\n","\n","\n","def load_from_mobilenet(net, checkpoint):\n","    source_state = checkpoint['state_dict']\n","    target_state = net.state_dict()\n","    new_target_state = collections.OrderedDict()\n","    for target_key, target_value in target_state.items():\n","        k = target_key\n","        if k.find('model') != -1:\n","            k = k.replace('model', 'module.model')\n","        if k in source_state and source_state[k].size() == target_state[target_key].size():\n","            new_target_state[target_key] = source_state[k]\n","        else:\n","            new_target_state[target_key] = target_state[target_key]\n","            print('[WARNING] Not found pre-trained parameters for {}'.format(target_key))\n","\n","    net.load_state_dict(new_target_state)\n"],"metadata":{"id":"n55mbsN2Bmm6","executionInfo":{"status":"ok","timestamp":1665977972598,"user_tz":-330,"elapsed":9,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["torch_model = PoseEstimationWithMobileNet()\n","checkpoint = torch.load('/content/checkpoint_iter_370000.pth')\n","load_state(torch_model, checkpoint)"],"metadata":{"id":"8JmYcvt4AZWl","executionInfo":{"status":"ok","timestamp":1665977994095,"user_tz":-330,"elapsed":3388,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yJGE9L4XAZU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input to the model\n","x = torch.randn(1, 3, 256, 456)\n","torch_out = torch_model(x)\n","input_names = ['data']\n","output_names = ['stage_0_output_1_heatmaps', 'stage_0_output_0_pafs',\n","                    'stage_1_output_1_heatmaps', 'stage_1_output_0_pafs']\n","\n","# Export the model\n","torch.onnx.export(torch_model,               # model being run\n","                  x,                         # model input (or a tuple for multiple inputs)\n","                  \"/content/human-pose-estimation.onnx\",   # where to save the model (can be a file or file-like object)\n","                  export_params=True,        # store the trained parameter weights inside the model file\n","                  opset_version=10,          # the ONNX version to export the model to\n","                  do_constant_folding=True,  # whether to execute constant folding for optimization\n","                  input_names=input_names,    # the model's input names\n","                output_names=output_names, # the model's output names\n","                  dynamic_axes={'input' : {0 : 'batch_size'},    # variable length axes\n","                                'output' : {0 : 'batch_size'}})"],"metadata":{"id":"CaaJsuMAy34W","executionInfo":{"status":"ok","timestamp":1665978260305,"user_tz":-330,"elapsed":8991,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["import onnx\n","\n","onnx_model = onnx.load(\"/content/human-pose-estimation.onnx\")\n","onnx.checker.check_model(onnx_model)"],"metadata":{"id":"_tBegcVgCtJi","executionInfo":{"status":"ok","timestamp":1665978379139,"user_tz":-330,"elapsed":573,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import onnxruntime\n","import numpy as np\n","\n","\n","ort_session = onnxruntime.InferenceSession(\"/content/human-pose-estimation.onnx\")\n","\n","def to_numpy(tensor):\n","    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n","\n","# compute ONNX Runtime output prediction\n","ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n","ort_outs = ort_session.run(None, ort_inputs)\n","\n","# compare ONNX Runtime and PyTorch results\n","np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n","\n","print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"ykr8DNsqDWyC","executionInfo":{"status":"error","timestamp":1665978478689,"user_tz":-330,"elapsed":5866,"user":{"displayName":"SOUMYADEEP BANIK","userId":"08525205833121109985"}},"outputId":"ca729b2c-74f4-4814-8c80-6ac8b88c2587"},"execution_count":20,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-718e98f38dd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# compare ONNX Runtime and PyTorch results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_allclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mort_outs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exported model has been tested with ONNXRuntime, and the result looks good!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-718e98f38dd1>\u001b[0m in \u001b[0;36mto_numpy\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# compute ONNX Runtime output prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'requires_grad'"]}]}]}